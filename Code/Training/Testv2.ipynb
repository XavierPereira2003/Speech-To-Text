{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 12:14:26.902973: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-03 12:14:27.063597: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-03 12:14:27.063628: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-03 12:14:27.064551: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-03 12:14:27.131327: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package punkt to /home/deon-xavier-\n",
      "[nltk_data]     pereira/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import librosa\n",
    "import torch\n",
    "import soundfile as sf\n",
    "from transformers import Wav2Vec2ForCTC,Wav2Vec2Tokenizer\n",
    "import evaluate\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \"\"\"\n",
    "    Returns the Wav2Vec2 tokienizer and the model from pretrained tokinezer model\n",
    "    \"\"\"\n",
    "    tokienizer=Wav2Vec2Tokenizer.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "    model=Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "    return tokienizer, model\n",
    "\n",
    "def Correction(input_text:str):\n",
    "    \"\"\"\n",
    "    Returns the input text with correction\n",
    "\n",
    "    Args:\n",
    "        input_text (str): Original text generated by the model\n",
    "    \"\"\"\n",
    "    sentence=nltk.sent_tokenize(input_text)\n",
    "    return (\" \".join([s.replace(s[0],s[0].capitalize(),1) for s in sentence]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe(tokenizer,model,input_file):\n",
    "    \"\"\"\n",
    "    Returns the transcribe text of the audio\n",
    "\n",
    "    Args:\n",
    "        tokiener (_type_): _description_\n",
    "        model (_type_): _description_\n",
    "        input_file (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    speech, sample_rate=sf.read(input_file)\n",
    "\n",
    "    if len(speech.shape)>1:\n",
    "        speech=speech[:,0]+speech[:,1]\n",
    "    \n",
    "    if sample_rate!=16000:\n",
    "        speech=librosa.resample(speech,orig_sr=sample_rate,target_sr=16000)\n",
    "    \n",
    "    \n",
    "    input_values = tokenizer(speech, return_tensors=\"pt\").input_values\n",
    "    logits=model(input_values).logits\n",
    "\n",
    "    predicted_ids=torch.argmax(logits,dim=-1)\n",
    "\n",
    "    transcription=tokenizer.decode(predicted_ids[0])\n",
    "\n",
    "    transcription=Correction(transcription.lower())\n",
    "    return transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "/home/deon-xavier-pereira/Desktop/Project/NNDL/nndl_env/lib/python3.10/site-packages/transformers/models/wav2vec2/tokenization_wav2vec2.py:733: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "wav_input =\"common_voice_en_38024625.mp3\" #\"84-121123-0000.flac\"\n",
    "tokenizer, model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The town ob line is served by the kohama county school district\n"
     ]
    }
   ],
   "source": [
    "text = transcribe(tokenizer,model,wav_input)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84-121123-0000.flac\n"
     ]
    }
   ],
   "source": [
    "print(wav_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585a202aaf534ccc89403f3a405554e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'predictions_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/deon-xavier-pereira/Desktop/Project/NNDL/Code/Training/Testv2.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/deon-xavier-pereira/Desktop/Project/NNDL/Code/Training/Testv2.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m cer \u001b[39m=\u001b[39m evaluate\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mcer\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/deon-xavier-pereira/Desktop/Project/NNDL/Code/Training/Testv2.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Evaluate\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/deon-xavier-pereira/Desktop/Project/NNDL/Code/Training/Testv2.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m wer_value \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(wer\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mpredictions_all, references\u001b[39m=\u001b[39mreferences_all), \u001b[39m4\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/deon-xavier-pereira/Desktop/Project/NNDL/Code/Training/Testv2.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m cer_value \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(cer\u001b[39m.\u001b[39mcompute(predictions\u001b[39m=\u001b[39mpredictions_all, references\u001b[39m=\u001b[39mreferences_all), \u001b[39m4\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/deon-xavier-pereira/Desktop/Project/NNDL/Code/Training/Testv2.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# Print results\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions_all' is not defined"
     ]
    }
   ],
   "source": [
    "# Load evaluators\n",
    "wer = evaluate.load('wer')\n",
    "cer = evaluate.load('cer')\n",
    "\n",
    "# Evaluate\n",
    "wer_value = round(wer.compute(predictions=predictions_all, references=references_all), 4)\n",
    "cer_value = round(cer.compute(predictions=predictions_all, references=references_all), 4)\n",
    "\n",
    "# Print results\n",
    "print('Final:')\n",
    "print(f'WER: {wer_value} | CER: {cer_value}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
